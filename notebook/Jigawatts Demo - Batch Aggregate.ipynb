{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe on raw_metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read\\\n",
    "   .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "   .options(keyspace = \"metrics\", table = \"raw_metrics\")\\\n",
    "   .load()\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the query plan and view some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- metric_time: timestamp (nullable = true)\n",
      " |-- metric_name: string (nullable = true)\n",
      " |-- metric_value: decimal(38,18) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@636b6ef0[device_id#0,metric_time#1,metric_name#2,metric_value#3]\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+--------------------+\n",
      "|device_id|         metric_time|metric_name|        metric_value|\n",
      "+---------+--------------------+-----------+--------------------+\n",
      "|        6|2016-04-05 18:35:...|        KwH|1.766843615010000000|\n",
      "|        6|2016-04-05 18:35:...|        KwH|1.738139198460000000|\n",
      "|        6|2016-04-05 18:35:...|        KwH|1.436533017840000000|\n",
      "|        6|2016-04-05 18:34:...|        KwH|1.208331618880000000|\n",
      "|        6|2016-04-05 18:34:...|        KwH|0.331787741071000000|\n",
      "|        6|2016-04-05 18:34:...|        KwH|0.525922436202000000|\n",
      "|        6|2016-04-05 18:33:...|        KwH|0.602361949655000000|\n",
      "|        6|2016-04-05 18:33:...|        KwH|0.187636462810000000|\n",
      "|        6|2016-04-05 18:33:...|        KwH|1.168336551980000000|\n",
      "|        6|2016-04-05 18:32:...|        KwH|1.456318619910000000|\n",
      "|        6|2016-04-05 18:32:...|        KwH|1.423352235120000000|\n",
      "|        6|2016-04-05 18:32:...|        KwH|1.784004004300000000|\n",
      "|        6|2016-04-05 18:31:...|        KwH|0.133582936767000000|\n",
      "|        6|2016-04-05 18:31:...|        KwH|0.129729083802000000|\n",
      "|        6|2016-04-05 18:31:...|        KwH|0.714167856414000000|\n",
      "|        6|2016-04-05 18:30:...|        KwH|0.863490678632000000|\n",
      "|        6|2016-04-05 18:30:...|        KwH|1.044096005350000000|\n",
      "|        6|2016-04-05 18:30:...|        KwH|0.626867294486000000|\n",
      "|        6|2016-04-05 18:29:...|        KwH|0.259369546527000000|\n",
      "|        6|2016-04-05 18:29:...|        KwH|1.772861344080000000|\n",
      "+---------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|device_id|\n",
      "+---------+\n",
      "|       50|\n",
      "|       51|\n",
      "|       52|\n",
      "|       53|\n",
      "|       54|\n",
      "|       55|\n",
      "|       56|\n",
      "|       57|\n",
      "|       58|\n",
      "|       59|\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|       60|\n",
      "|        5|\n",
      "|       61|\n",
      "|        6|\n",
      "|       62|\n",
      "|        7|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"device_id\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|device_id|count|\n",
      "+---------+-----+\n",
      "|       50|  149|\n",
      "|       51|  149|\n",
      "|       52|  149|\n",
      "|       53|  149|\n",
      "|       54|  149|\n",
      "|       55|  149|\n",
      "|       56|  149|\n",
      "|       57|  149|\n",
      "|       58|  149|\n",
      "|       59|  149|\n",
      "|        1|  149|\n",
      "|        2|  149|\n",
      "|        3|  149|\n",
      "|       60|  149|\n",
      "|        4|  149|\n",
      "|        5|  149|\n",
      "|       61|  149|\n",
      "|        6|  149|\n",
      "|       62|  149|\n",
      "|        7|  149|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"device_id\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import datetime\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set time variables for date filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time = datetime.datetime.now()\n",
    "epochtime = int(time.strftime(\"%s\"))\n",
    "start_time = epochtime - 86400\n",
    "compare_time = datetime.datetime.fromtimestamp(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a dataframe from the raw metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawmetrics = sqlContext.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"raw_metrics\", keyspace=\"metrics\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter metrics to those in last 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+------------+\n",
      "|device_id|         metric_time|metric_name|metric_value|\n",
      "+---------+--------------------+-----------+------------+\n",
      "|        1|2016-03-31 23:43:...|        KWH|   1.5783356|\n",
      "|        1|2016-03-31 23:28:...|        KWH|   1.0391227|\n",
      "|        1|2016-03-31 23:13:...|        KWH|  0.26933274|\n",
      "|        1|2016-03-31 22:58:...|        KWH|  0.10949156|\n",
      "|        1|2016-03-31 22:43:...|        KWH|   1.5273823|\n",
      "|        1|2016-03-31 22:28:...|        KWH|  0.10674351|\n",
      "|        1|2016-03-31 22:13:...|        KWH|   1.7821758|\n",
      "|        1|2016-03-31 21:58:...|        KWH|   0.9134149|\n",
      "|        1|2016-03-31 21:43:...|        KWH|   1.3860309|\n",
      "|        1|2016-03-31 21:28:...|        KWH|   1.4682286|\n",
      "|        1|2016-03-31 21:13:...|        KWH|   0.8666165|\n",
      "|        1|2016-03-31 20:58:...|        KWH|   1.0810951|\n",
      "|        1|2016-03-31 20:43:...|        KWH|   1.4882127|\n",
      "|        1|2016-03-31 20:28:...|        KWH|  0.43541336|\n",
      "|        1|2016-03-31 20:13:...|        KWH|   1.5310488|\n",
      "|        1|2016-03-31 19:58:...|        KWH|  0.16667736|\n",
      "|        1|2016-03-31 19:43:...|        KWH|   1.6417416|\n",
      "|        1|2016-03-31 19:28:...|        KWH|   1.0944177|\n",
      "|        1|2016-03-31 19:13:...|        KWH|  0.38889095|\n",
      "|        1|2016-03-31 18:58:...|        KWH|   1.3758928|\n",
      "+---------+--------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_day = rawmetrics.where(rawmetrics.metric_time > compare_time)\n",
    "last_day.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "averages = last_day.groupby('device_id').agg(func.avg('metric_value').alias('metric_avg'))\n",
    "maximums = last_day.groupby('device_id').agg(func.max('metric_value').alias('metric_max'))\n",
    "minimums = last_day.groupby('device_id').agg(func.min('metric_value').alias('metric_min'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rename id columns for uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "averages_a = averages.withColumnRenamed(\"device_id\", \"id\")\n",
    "maximums_a = maximums.withColumnRenamed(\"device_id\", \"maxid\")\n",
    "minimums_a = minimums.withColumnRenamed(\"device_id\", \"minid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join the tables above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = averages_a.join(maximums_a, averages_a.id == maximums_a.maxid)\n",
    "aggs = temp.join(minimums, temp.id == minimums.device_id).select('id','metric_min','metric_max','metric_avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add columns to format for cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addday = aggs.withColumn(\"metric_day\", lit(time))\n",
    "addname = addday.withColumn(\"metric_name\",lit(\"KWH\"))\n",
    "inserts = addname.withColumnRenamed(\"id\",\"device_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to a new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inserts.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "    options(table=\"daily_rollups\", keyspace = \"metrics\").save(mode =\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark DSE Cluster",
   "language": "python",
   "name": "pyspark-dse-cluster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
